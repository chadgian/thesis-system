<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Web-Based Document Similarity Tool</title>
    <link rel="stylesheet" type="text/css" href="{{ url_for('static', filename='styles.css') }}">
    <script src="{{ url_for('static', filename='script.js') }}"></script>
</head>
<body>
    <div class="header">
        <h2><a href="{{ url_for('home') }}" style="text-decoration: none; color: #f6f6f2;">Web-Based Document Similarity Tool</a></h2>
        <div class="header-menu">
            <div class="dropdown">
                <a class="dropdown-toggle" id="menu-header-link">Algorithms</a>
                <div class="dropdown-menu">
                    <a href="{{ url_for('cosine') }}">Cosine Similarity</a>
                    <a href="{{ url_for('tfidf') }}">TF-IDF Vectorizer</a>
                </div>
            </div>
            <a href="{{ url_for('about') }}" id="menu-header-link">About</a>
            <a href="{{ url_for('contactus') }}" id="menu-header-link">Contact Us</a>
        </div>
    </div>
    <center><div class="cosine-body">
        <h1>What is Cosine Similarity?</h1>
        <p class="cosine-intro">Cosine similarity is a measurement that quantifies the similarity between two or more vectors. 
            It's the cosine of the angle between vectors or documents in this case, which are typically non-zero and within an 
            inner product space. <a href="#Built In" >[1]</a></p>
        <div style="max-width: 90%; margin-left: auto; margin-right: auto;">
            <h3>Application of Cosine Similarity:</h3>
            <ul>
                <li><b>Document Representation</b></li>
                    <ul>
                        <li>
                            Documents, in the context of our web app, are transformed into numerical vectors 
                            in a high-dimensional space. Each dimension represents a unique term or word found 
                            in the corpus of documents.
                        </li>
                    </ul>
                <li><b>Text Vectorization</b></li>
                    <ul>
                        <li>
                            Text vectorization is the process of converting textual data into a format 
                            suitable for mathematical analysis. Methods like TF-IDF are employed to 
                            create numerical representations of documents, facilitating comparison.
                        </li>
                    </ul>
                <li><b>Cosine Similarity as a Measure</b></li>
                    <ul>
                        <li>
                            Cosine Similarity is a metric used to quantify the similarity between two vectors. 
                            In document similarity analysis, it measures the cosine of the angle between the 
                            vectors, providing a reliable measure of content similarity.
                        </li>
                    </ul>
                <li><b>Document Matching</b></li>
                    <ul>
                        <li>
                            The primary goal in document similarity analysis is to find documents 
                            that are most similar to a reference document. Cosine Similarity plays 
                            a key role in ranking and identifying such documents based on their content similarity.
                        </li>
                    </ul>
                <li><b>Information Retrieval</b></li>
                    <ul>
                        <li>
                            In information retrieval systems, 
                            Cosine Similarity is instrumental 
                            in retrieving relevant documents from a large dataset. 
                            It ensures that documents with similar content are prioritized in the search results.
                        </li>
                    </ul>
                <li><b>Search Engines</b></li>
                    <ul>
                        <li>
                            Search engines leverage Cosine Similarity to 
                            improve the accuracy of search results. By considering 
                            the content similarity between user queries and documents, 
                            search engines can provide more relevant and useful results.
                        </li>
                    </ul>
                <li><b>Document Clustering</b></li>
                    <ul>
                        <li>
                            Cosine Similarity is applied in document clustering to group 
                            together documents with similar content. This aids in organizing 
                            large datasets and enables efficient analysis of related documents.
                        </li>
                    </ul>
                <li><b>Recommendation Systems</b></li>
                    <ul>
                        <li>
                            In recommendation systems, Cosine Similarity is utilized to suggest 
                            documents that are similar to a user's preferences. This approach 
                            enhances user experience by offering content that aligns with their interests.
                        </li>
                    </ul>
                <li><b>Real World Examples</b></li>
                    <ul>
                        <li>
                            Real-world examples showcase the practical benefits of document 
                            similarity analysis using Cosine Similarity. Whether in academic 
                            research, content curation, or business intelligence, this approach 
                            has proven valuable in various domains.
                        </li>
                    </ul>
            </ul>
            <div style="font-size: 16px;">
                <h3>Mathematical Representation:</h3>
                <p style="line-height: 1;">Formula: </p>
                <p><img src="{{ url_for('static', filename='img/formula-cosine.png') }}" alt="Cosine Similarity Formula" 
                    max-width="30%" height="auto"></p>
                <p><u>Example:</u></p>
                <p>Consider two documents:</p><p>Doc1 = "This is the first example of document"</p><p>Doc2 = "Second document is this sentence"</p>
                <p>Before calculating for the similarity, the two documents will first be vectorized using Count Vectorizer. 
                    <i>The CountVectorizer is a valuable utility offered by the scikit-learn library in Python, 
                    proves effective in converting a given text into a vector, considering the frequency (count) 
                    of each word present in the entire text. <a href="#GeeksforGeeks">[2]</a></i>
                </p>
                <p style="text-align: center;"><small>Count Vectorizer Table of Doc1 and Doc2</small><img src="{{ url_for('static', filename='img/count-vectorizer-table.png') }}" alt="Count Vectorizer Table of Doc1 and Doc2"></p>
                <p>Now that we vectorized the two documents, it is time to calculate its cosine similarity. <i>We will follow the formula given above.</i></p>
                <p>To further analyze and understand the formula, we will separate the solving process for each part.</p>
                <p style="text-align: center;"><img src="{{ url_for('static', filename='img/formula-process-cosine.png') }}" alt="Formula Process" max-width="100%" height="auto">
                    <img src="{{ url_for('static', filename='img/cosine-result.png') }}" alt="Result" max-width="100%" height="auto"></p> 
                <p>The result after calculating the cosine similarity of two documents is <b>0.5071.</b> The cosine similarity outputs values from 0 to 1. 1 implies 100% identical document and 0 means no similarity. Given that the result of Doc1 and Doc2 cosine similarity is 0.5071, it means that the two documents are approximately <b>51% similar to each other.</b></p>
            </div>
            <div>
                <h3>Advantages of Cosine Similarity:</h3>
                <ol>
                    <li><b>1. Robust to Document Length:</b>
                        <ul>
                            <li>Cosine similarity is independent of the length of the documents <a href="#Wikipedia">[4]</a>. This is because it focuses on the relative importance of words rather than their absolute frequencies. This makes it suitable for comparing documents of different sizes, avoiding bias towards longer documents <a href="#Medium">[3]</a>.</li>
                        </ul>
                    </li>
                    <li><b>2. Efficient Computation:</b> 
                        <ul>
                            <li>Cosine similarity can be efficiently computed, especially for sparse vectors where only a few elements have non-zero values <a href="#Wikipedia">[4]</a>. This makes it scalable for comparing large datasets of documents.</li>
                        </ul>
                    </li>
                    <li><b>3. Easy Interpretation:</b> 
                        <ul>
                            <li>Cosine similarity produces a score between 0 and 1, where 0 represents no similarity and 1 represents perfect similarity. This makes the results easy to interpret and use for various applications <a href="#Wikipedia">[4]</a>.</li>
                        </ul>
                    </li>
                    <li><b>4. Widely Used:</b> 
                        <ul>
                            <li>Cosine similarity is a well-established technique with numerous applications in information retrieval, text mining, natural language processing, and recommendation systems <a href="#Machine Learning Plus">[5]</a>. This vast usage provides a strong foundation for further research and development of related techniques.</li>
                        </ul>
                    </li>
                </ol>
            </div>
        </div>
        <h2>Conclusion</h2>
        <p class="cosine-intro">Cosine similarity is a powerful and versatile tool for measuring the similarity between documents. It offers several advantages, including robustness to document length, efficient computation, easy interpretation, and widespread usage. This makes it a valuable technique for various applications, including information retrieval, search engines, document clustering, recommendation systems, and more.</p>
        <p class="cosine-intro"> With its ability to efficiently compare documents, cosine similarity promises to be a cornerstone in document analysis and information processing for years to come. Further research and development in this area will continue to enhance its capabilities and expand its applications.</p>
        <h2>References</h2>
        <section id="Built In"><p>[1] “Understanding Cosine Similarity and Its Application | Built In,” builtin.com. <a href="https://builtin.com/machine-learning/cosine-similarity">https://builtin.com/machine-learning/cosine-similarity</a></p></section>
        <section id="GeeksforGeeks"><p>[2] “Using CountVectorizer to Extracting Features from Text,” GeeksforGeeks, Jul. 15, 2020. <a href="https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/">https://www.geeksforgeeks.org/using-countvectorizer-to-extracting-features-from-text/</a></p></section>
        <section id="Medium"><p>[3] F. Vidal, “Similarity Distances for Natural Language Processing,” Medium, Jun. 11, 2021. <a href="https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55">https://flavien-vidal.medium.com/similarity-distances-for-natural-language-processing-16f63cd5ba55</a></p></section>
        <section id="Wikipedia"><p>[4] “Cosine similarity - Wikipedia,” static.hlt.bme.hu. <a href="https://static.hlt.bme.hu/semantics/external/pages/n-gram/en.wikipedia.org/wiki/Cosine_similarity.html">https://static.hlt.bme.hu/semantics/external/pages/n-gram/en.wikipedia.org/wiki/Cosine_similarity.html</a> (accessed Dec. 10, 2023).</p></section>
        <section id="Machine Learning Plus"><p>[5] S. Prabhakaran, “Cosine Similarity – Understanding the math and how it works (with python codes),” Machine Learning Plus, Oct. 22, 2018. <a href="https://www.machinelearningplus.com/nlp/cosine-similarity/">https://www.machinelearningplus.com/nlp/cosine-similarity/</a></p></section>
    </div></center>
    <footer>
        <p>&copy; 2023 Web-Based Document Similarity Tool Using TF-IDF and Cosine Similarity Algorithm</p>
    </footer>
</body>
</html>
